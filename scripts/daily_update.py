#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¯æ—¥æ›´æ–°è„šæœ¬ï¼šè·å–æœ€æ–°æ•°æ®ã€è®¡ç®—å› å­ã€ç”Ÿæˆé¢„æµ‹å’Œæƒé‡

åŠŸèƒ½ï¼š
1. è·å–æœ€è¿‘ N å¤©çš„ä»·æ ¼æ•°æ®ï¼ˆé»˜è®¤ 60 å¤©ï¼Œå› ä¸ºå› å­è®¡ç®—éœ€è¦å†å²æ•°æ®ï¼‰
2. æ›´æ–°å› å­åº“ï¼ˆåªè®¡ç®—æœ€æ–°æ—¥æœŸçš„å› å­ï¼‰
3. åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
4. ç”Ÿæˆæœ€æ–°æ—¥æœŸçš„æƒé‡

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/daily_update.py

å‚æ•°ï¼š
    --lookback-days: è·å–å¤šå°‘å¤©çš„å†å²æ•°æ®ï¼ˆé»˜è®¤ 60ï¼Œå› å­è®¡ç®—éœ€è¦ï¼‰
    --model-type: æ¨¡å‹ç±»å‹ (lightgbm/catboost/xgboostï¼Œé»˜è®¤ lightgbm)
    --skip-fetch: è·³è¿‡æ•°æ®è·å–ï¼ˆå¦‚æœå·²ç»æ‰‹åŠ¨æ›´æ–°äº†æ•°æ®ï¼‰
    --skip-factors: è·³è¿‡å› å­è®¡ç®—ï¼ˆå¦‚æœå› å­å·²ç»è®¡ç®—å¥½ï¼‰
"""

import argparse
import json
import sys
from pathlib import Path
from datetime import datetime, timedelta
import warnings

import pandas as pd
import numpy as np
import yaml

# ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„ç®¡ç†
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config.path import SETTINGS_FILE, get_path
from src.data_pipeline import (
    load_settings,
    get_tickers_from_qlib,
    fetch_daily_prices,
    fetch_daily_prices_from_qlib,
)
from src.factor_engine import (
    read_prices,
    calculate_all_factors,
    qlib_style_processing,
)
from src.optimizer import (
    topk_dropout_strategy,
    full_rebalance_strategy,
    load_predictions,
    run_optimize,
)

warnings.filterwarnings("ignore")

SETTINGS = SETTINGS_FILE


def load_top100_tickers() -> list:
    """åŠ è½½å¸‚å€¼å‰100è‚¡ç¥¨åˆ—è¡¨"""
    top100_file = get_path("data/top100_stocks.txt")
    if top100_file.exists():
        with open(top100_file, "r") as f:
            tickers = [line.strip() for line in f if line.strip()]
        print(f"[Info] åŠ è½½å‰100è‚¡ç¥¨åˆ—è¡¨: {len(tickers)} åªè‚¡ç¥¨")
        return tickers
    else:
        print(f"[Warn] æ‰¾ä¸åˆ°å‰100è‚¡ç¥¨æ–‡ä»¶: {top100_file}")
        print(f"       å°†ä½¿ç”¨æ‰€æœ‰è‚¡ç¥¨ï¼ˆå»ºè®®è¿è¡Œ python get_top100_stocks.py ç”Ÿæˆåˆ—è¡¨ï¼‰")
        return None


def get_latest_date_from_prices(prices_path: Path) -> pd.Timestamp:
    """ä»ä»·æ ¼æ–‡ä»¶ä¸­è·å–æœ€æ–°æ—¥æœŸ"""
    if not prices_path.exists():
        return None
    
    prices = pd.read_parquet(prices_path)
    if isinstance(prices.index, pd.MultiIndex):
        dates = prices.index.get_level_values("date")
    else:
        # å°è¯•ä»ç´¢å¼•ä¸­æå–æ—¥æœŸ
        dates = pd.to_datetime(prices.index.get_level_values(0), errors='coerce')
    
    if len(dates) == 0:
        return None
    
    return pd.to_datetime(dates.max())


def update_prices(cfg, lookback_days: int = 60):
    """æ›´æ–°ä»·æ ¼æ•°æ®ï¼ˆåªè·å–æœ€è¿‘ N å¤©ï¼‰"""
    print("=" * 60)
    print("[Step 1] æ›´æ–°ä»·æ ¼æ•°æ®")
    print("=" * 60)
    
    prices_path = Path(cfg["paths"]["prices_parquet"])
    
    # è·å–ç°æœ‰æ•°æ®çš„æœ€æ–°æ—¥æœŸ
    latest_date = get_latest_date_from_prices(prices_path)
    
    if latest_date is not None:
        print(f"  ç°æœ‰æ•°æ®æœ€æ–°æ—¥æœŸ: {latest_date.date()}")
        # ä»æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹è·å–
        start_date = (latest_date + timedelta(days=1)).strftime("%Y-%m-%d")
    else:
        # å¦‚æœæ²¡æœ‰ç°æœ‰æ•°æ®ï¼Œè·å–æœ€è¿‘ N å¤©
        start_date = (datetime.now() - timedelta(days=lookback_days)).strftime("%Y-%m-%d")
        print(f"  æœªæ‰¾åˆ°ç°æœ‰æ•°æ®ï¼Œè·å–æœ€è¿‘ {lookback_days} å¤©çš„æ•°æ®")
    
    end_date = datetime.now().strftime("%Y-%m-%d")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    if latest_date is not None:
        days_since_update = (datetime.now() - latest_date.to_pydatetime()).days
        if days_since_update == 0:
            print("  âœ“ æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œè·³è¿‡æ›´æ–°")
            return
        print(f"  éœ€è¦æ›´æ–° {days_since_update} å¤©çš„æ•°æ®")
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    region = cfg.get("data", {}).get("region", "us")
    instruments = cfg.get("data", {}).get("instruments", "sp500")
    
    if prices_path.exists():
        # ä»ç°æœ‰æ•°æ®ä¸­è·å–è‚¡ç¥¨åˆ—è¡¨
        existing_prices = pd.read_parquet(prices_path)
        if isinstance(existing_prices.index, pd.MultiIndex):
            tickers = sorted(existing_prices.index.get_level_values("ticker").unique().tolist())
        else:
            tickers = get_tickers_from_qlib(instruments, region)
        print(f"  ä»ç°æœ‰æ•°æ®ä¸­è·å– {len(tickers)} åªè‚¡ç¥¨")
    else:
        tickers = get_tickers_from_qlib(instruments, region)
        print(f"  è·å– {len(tickers)} åªè‚¡ç¥¨")
    
    # è·å–æ–°æ•°æ®
    print(f"  è·å–æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
    
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„ç”¨äºè·å–æ–°æ•°æ®
        temp_path = prices_path.parent / f"temp_{prices_path.name}"
        
        if region == "cn":
            new_data = fetch_daily_prices_from_qlib(instruments, start_date, end_date, str(temp_path), region)
        else:
            new_data = fetch_daily_prices(tickers, start_date, end_date, str(temp_path), region)
        
        # å‡½æ•°å·²ç»ä¿å­˜äº†æ•°æ®åˆ° temp_pathï¼Œç°åœ¨è¯»å–å¹¶åˆå¹¶
        if temp_path.exists():
            new_data = pd.read_parquet(temp_path)
            temp_path.unlink()  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            
            if new_data.empty:
                print("  âš ï¸ æ²¡æœ‰è·å–åˆ°æ–°æ•°æ®")
                return
            
            # åˆå¹¶æ•°æ®
            if prices_path.exists() and latest_date is not None:
                existing_prices = pd.read_parquet(prices_path)
                # ç¡®ä¿ç´¢å¼•æ ¼å¼ä¸€è‡´
                if isinstance(existing_prices.index, pd.MultiIndex):
                    existing_prices.index = pd.MultiIndex.from_tuples(
                        [(pd.to_datetime(d), t) for d, t in existing_prices.index],
                        names=["date", "ticker"]
                    )
                
                # åˆå¹¶ï¼šä¿ç•™æ—§æ•°æ®ï¼Œæ·»åŠ æ–°æ•°æ®
                combined = pd.concat([existing_prices, new_data]).drop_duplicates().sort_index()
                combined.to_parquet(prices_path)
                print(f"  âœ“ å·²åˆå¹¶æ•°æ®ï¼Œæ€»è¡Œæ•°: {len(combined)}")
            else:
                new_data.to_parquet(prices_path)
                print(f"  âœ“ å·²ä¿å­˜æ–°æ•°æ®ï¼Œæ€»è¡Œæ•°: {len(new_data)}")
        else:
            print("  âš ï¸ æ•°æ®è·å–å¤±è´¥ï¼Œä¸´æ—¶æ–‡ä»¶ä¸å­˜åœ¨")
            
    except ValueError as e:
        error_msg = str(e)
        # æ£€æŸ¥æ˜¯å¦æ˜¯"æ²¡æœ‰æ•°æ®"çš„é”™è¯¯ï¼ˆå¯èƒ½æ˜¯éäº¤æ˜“æ—¥ï¼‰
        if "No data downloaded" in error_msg:
            print(f"  âš ï¸ æ•°æ®è·å–å¤±è´¥: {error_msg}")
            print(f"\n  å¯èƒ½åŸå› ï¼š")
            print(f"    1. {end_date} å¯èƒ½æ˜¯éäº¤æ˜“æ—¥ï¼ˆèŠ‚å‡æ—¥ï¼‰")
            print(f"    2. yfinance API æš‚æ—¶ä¸å¯ç”¨")
            print(f"    3. æ•°æ®å°šæœªå‡†å¤‡å¥½ï¼ˆé€šå¸¸éœ€è¦äº¤æ˜“æ—¥ç»“æŸåï¼‰")
            print(f"\n  ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
            if latest_date is not None:
                print(f"    - å½“å‰æ•°æ®æœ€æ–°æ—¥æœŸ: {latest_date.date()}")
            print(f"    - è·³è¿‡æ•°æ®è·å–ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®ç»§ç»­ï¼š")
            print(f"      python scripts/daily_update.py --skip-fetch")
            print(f"    - æˆ–ç­‰å¾…ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥å†å°è¯•å®Œæ•´æ›´æ–°")
            # å¦‚æœæ˜¯æ•°æ®ä¸å¯ç”¨ï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸è·³è¿‡ç»§ç»­
            return
        else:
            print(f"  âœ— æ•°æ®è·å–å¤±è´¥: {e}")
            raise
    except Exception as e:
        print(f"  âœ— æ•°æ®è·å–å¤±è´¥: {e}")
        print(f"\n  ğŸ’¡ æç¤ºï¼šå¯ä»¥ä½¿ç”¨ --skip-fetch è·³è¿‡æ•°æ®è·å–ï¼Œç»§ç»­ä½¿ç”¨ç°æœ‰æ•°æ®")
        raise


def update_factors(cfg, lookback_days: int = 60):
    """æ›´æ–°å› å­ï¼ˆåªè®¡ç®—æœ€æ–°æ—¥æœŸçš„å› å­ï¼Œä½†éœ€è¦å†å²æ•°æ®ï¼‰"""
    print("\n" + "=" * 60)
    print("[Step 2] æ›´æ–°å› å­")
    print("=" * 60)
    
    prices_path = Path(cfg["paths"]["prices_parquet"])
    factors_path = Path(cfg["paths"]["factors_store"])
    
    if not prices_path.exists():
        raise FileNotFoundError(f"ä»·æ ¼æ–‡ä»¶ä¸å­˜åœ¨: {prices_path}")
    
    # è¯»å–ä»·æ ¼æ•°æ®
    prices = read_prices(cfg)
    
    # è¿‡æ»¤åˆ°å‰100è‚¡ç¥¨ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    top100_tickers = load_top100_tickers()
    if top100_tickers:
        available_tickers = set(prices.index.get_level_values("ticker").unique())
        valid_tickers = [t for t in top100_tickers if t in available_tickers]
        if len(valid_tickers) < len(top100_tickers):
            missing = set(top100_tickers) - available_tickers
            print(f"  [Warn] {len(missing)} åªå‰100è‚¡ç¥¨åœ¨ä»·æ ¼æ•°æ®ä¸­ä¸å­˜åœ¨: {sorted(list(missing))[:10]}...")
        prices = prices.loc[prices.index.get_level_values("ticker").isin(valid_tickers)]
        print(f"  [Info] è¿‡æ»¤åˆ°å‰100è‚¡ç¥¨: {len(valid_tickers)} åªè‚¡ç¥¨")
    
    # è·å–æœ€æ–°æ—¥æœŸ
    dates = prices.index.get_level_values("date")
    latest_date = pd.to_datetime(dates.max())
    
    # æ£€æŸ¥å› å­åº“ä¸­æ˜¯å¦å·²æœ‰æœ€æ–°æ—¥æœŸçš„å› å­
    existing_factors = None
    if factors_path.exists():
        existing_factors = pd.read_parquet(factors_path)
        # ç¡®ä¿ç´¢å¼•æ ¼å¼ä¸€è‡´
        if isinstance(existing_factors.index, pd.MultiIndex):
            existing_factors.index = pd.MultiIndex.from_tuples(
                [(pd.to_datetime(d), t) for d, t in existing_factors.index],
                names=["date", "ticker"]
            )
        
        existing_dates = existing_factors.index.get_level_values("date")
        if len(existing_dates) > 0:
            latest_existing_date = pd.to_datetime(existing_dates.max())
            
            # å¦‚æœæœ€æ–°æ—¥æœŸçš„å› å­å·²å­˜åœ¨ï¼Œè·³è¿‡è®¡ç®—
            if latest_existing_date >= latest_date:
                print(f"  âœ“ å› å­å·²æ˜¯æœ€æ–°ï¼ˆæœ€æ–°æ—¥æœŸ: {latest_date.date()}ï¼‰")
                print(f"    å› å­åº“æœ€æ–°æ—¥æœŸ: {latest_existing_date.date()}")
                return
    
    # è®¡ç®—éœ€è¦çš„å†å²æ•°æ®èŒƒå›´ï¼ˆå› å­è®¡ç®—éœ€è¦å†å²æ•°æ®ï¼‰
    # æ³¨æ„ï¼šè™½ç„¶åªä¿å­˜æœ€æ–°æ—¥æœŸï¼Œä½†è®¡ç®—æ—¶éœ€è¦å†å²æ•°æ®ä½œä¸ºè¾“å…¥
    # å¢åŠ æ•°æ®èŒƒå›´ä»¥ç¡®ä¿ TA-Lib å› å­è®¡ç®—æœ‰è¶³å¤Ÿæ•°æ®ï¼ˆTA-Lib éœ€è¦è‡³å°‘ 30-50 å¤©ï¼‰
    extended_lookback = max(lookback_days, 90)  # è‡³å°‘ 90 å¤©ä»¥ç¡®ä¿ TA-Lib æœ‰è¶³å¤Ÿæ•°æ®
    start_date = (latest_date - timedelta(days=extended_lookback)).strftime("%Y-%m-%d")
    end_date = latest_date.strftime("%Y-%m-%d")
    
    print(f"  è®¡ç®—å› å­æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
    print(f"  ï¼ˆéœ€è¦å†å²æ•°æ®ç”¨äºå› å­è®¡ç®—ï¼Œä½†åªä¿å­˜æœ€æ–°æ—¥æœŸ {latest_date.date()} çš„å› å­ï¼‰")
    print(f"  ï¼ˆæ‰©å±•æ•°æ®èŒƒå›´åˆ° {extended_lookback} å¤©ä»¥ç¡®ä¿ TA-Lib å› å­è®¡ç®—æœ‰è¶³å¤Ÿæ•°æ®ï¼‰")
    
    # è®¡ç®—å› å­ï¼ˆä¼šè®¡ç®—æ•´ä¸ªèŒƒå›´ï¼Œä½†åªä¿å­˜æœ€æ–°æ—¥æœŸï¼‰
    try:
        # åªä½¿ç”¨éœ€è¦çš„å†å²æ•°æ®èŒƒå›´æ¥è®¡ç®—å› å­
        # è¿™æ ·è™½ç„¶ä¼šè®¡ç®—æ•´ä¸ªèŒƒå›´ï¼Œä½†å› å­è®¡ç®—å‡½æ•°å†…éƒ¨éœ€è¦å†å²æ•°æ®
        new_factors = calculate_all_factors(prices, start_date, end_date)
        
        # å¤„ç†å› å­ï¼ˆå¯¹é½è®­ç»ƒæ—¶çš„å¤„ç†æ–¹å¼ï¼‰
        print("  å¤„ç†å› å­ï¼ˆwinsorize + zscoreï¼‰...")
        processed_factors = new_factors.copy()
        
        for col in processed_factors.columns:
            if processed_factors[col].notna().sum() > 0:
                processed_factors[col] = qlib_style_processing(processed_factors[col])
        
        # åªä¿ç•™æœ€æ–°æ—¥æœŸçš„å› å­ï¼ˆé¿å…é‡å¤ä¿å­˜å†å²æ•°æ®ï¼‰
        latest_factors = processed_factors.loc[
            processed_factors.index.get_level_values("date") == latest_date
        ]
        
        if len(latest_factors) == 0:
            print(f"  âš ï¸ æœ€æ–°æ—¥æœŸ {latest_date.date()} çš„å› å­è®¡ç®—å¤±è´¥")
            return
        
        # åˆå¹¶åˆ°ç°æœ‰å› å­åº“
        if factors_path.exists():
            # ç§»é™¤ç°æœ‰æ•°æ®ä¸­çš„æœ€æ–°æ—¥æœŸï¼ˆå¦‚æœæœ‰ï¼Œé¿å…é‡å¤ï¼‰
            existing_factors = existing_factors.loc[
                existing_factors.index.get_level_values("date") < latest_date
            ]
            
            # è¿‡æ»¤ç°æœ‰å› å­åº“åˆ°å‰100è‚¡ç¥¨ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if top100_tickers:
                existing_factors = existing_factors.loc[
                    existing_factors.index.get_level_values("ticker").isin(valid_tickers)
                ]
                print(f"  [Info] è¿‡æ»¤ç°æœ‰å› å­åº“åˆ°å‰100è‚¡ç¥¨: {len(valid_tickers)} åªè‚¡ç¥¨")
            
            # åˆå¹¶
            combined = pd.concat([existing_factors, latest_factors]).sort_index()
            
            # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿åªåŒ…å«å‰100è‚¡ç¥¨
            if top100_tickers:
                combined = combined.loc[
                    combined.index.get_level_values("ticker").isin(valid_tickers)
                ]
            
            combined.to_parquet(factors_path)
            print(f"  âœ“ å·²æ›´æ–°å› å­ï¼Œæœ€æ–°æ—¥æœŸ: {latest_date.date()}")
            print(f"    æ€»è¡Œæ•°: {len(combined)}, å› å­æ•°: {len(combined.columns)}")
            print(f"    è‚¡ç¥¨æ•°: {combined.index.get_level_values('ticker').nunique()}")
            print(f"    æ—¥æœŸèŒƒå›´: {combined.index.get_level_values('date').min().date()} åˆ° {combined.index.get_level_values('date').max().date()}")
        else:
            # å¦‚æœæ²¡æœ‰ç°æœ‰å› å­åº“ï¼Œåªä¿å­˜æœ€æ–°æ—¥æœŸ
            latest_factors.to_parquet(factors_path)
            print(f"  âœ“ å·²ä¿å­˜å› å­ï¼Œæœ€æ–°æ—¥æœŸ: {latest_date.date()}")
            print(f"    æ€»è¡Œæ•°: {len(latest_factors)}, å› å­æ•°: {len(latest_factors.columns)}")
            print(f"    è‚¡ç¥¨æ•°: {latest_factors.index.get_level_values('ticker').nunique()}")
            
    except Exception as e:
        print(f"  âœ— å› å­è®¡ç®—å¤±è´¥: {e}")
        raise


def generate_daily_prediction(cfg, model_type: str = "lightgbm"):
    """ç”Ÿæˆæœ€æ–°æ—¥æœŸçš„é¢„æµ‹"""
    print("\n" + "=" * 60)
    print(f"[Step 3] ç”Ÿæˆé¢„æµ‹ï¼ˆæ¨¡å‹: {model_type}ï¼‰")
    print("=" * 60)
    
    model_dir = Path(cfg["paths"]["model_dir"])
    factors_path = Path(cfg["paths"]["factors_store"])
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    ranker_path = model_dir / "lgbm_ranker.txt"
    reg_path = model_dir / "lgbm_regression.txt"
    
    if not ranker_path.exists() and not reg_path.exists():
        raise FileNotFoundError(
            f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œè®­ç»ƒ: python src/modeling.py --train"
        )
    
    # åŠ è½½å› å­æ•°æ®
    print("  åŠ è½½å› å­æ•°æ®...")
    factor_store = pd.read_parquet(factors_path)
    
    # è¿‡æ»¤åˆ°å‰100è‚¡ç¥¨ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    top100_tickers = load_top100_tickers()
    if top100_tickers:
        available_tickers = set(factor_store.index.get_level_values("ticker").unique())
        valid_tickers = [t for t in top100_tickers if t in available_tickers]
        if len(valid_tickers) < len(top100_tickers):
            missing = set(top100_tickers) - available_tickers
            print(f"  [Warn] {len(missing)} åªå‰100è‚¡ç¥¨åœ¨å› å­æ•°æ®ä¸­ä¸å­˜åœ¨: {sorted(list(missing))[:10]}...")
        factor_store = factor_store.loc[factor_store.index.get_level_values("ticker").isin(valid_tickers)]
        print(f"  [Info] è¿‡æ»¤åˆ°å‰100è‚¡ç¥¨: {len(valid_tickers)} åªè‚¡ç¥¨")
    
    # ä¿®å¤ç´¢å¼•æ ¼å¼
    if isinstance(factor_store.index, pd.MultiIndex):
        level_0 = factor_store.index.get_level_values(0)
        level_1 = factor_store.index.get_level_values(1)
        if pd.api.types.is_datetime64_any_dtype(level_0):
            dates = pd.to_datetime(level_0, errors='coerce')
            tickers = pd.Series(level_1).astype(str).values
        elif pd.api.types.is_datetime64_any_dtype(level_1):
            dates = pd.to_datetime(level_1, errors='coerce')
            tickers = pd.Series(level_0).astype(str).values
        else:
            dates = pd.to_datetime(level_0, errors='coerce')
            tickers = pd.Series(level_1).astype(str).values
        factor_store.index = pd.MultiIndex.from_arrays([dates, tickers], names=["date", "ticker"])
    
    # è·å–æœ€æ–°æ—¥æœŸ
    dates = factor_store.index.get_level_values("date")
    latest_date = pd.to_datetime(dates.max())
    print(f"  æœ€æ–°å› å­æ—¥æœŸ: {latest_date.date()}")
    
    # è·å–æœ€æ–°æ—¥æœŸçš„å› å­
    latest_factors = factor_store.loc[factor_store.index.get_level_values("date") == latest_date].copy()
    
    if len(latest_factors) == 0:
        raise ValueError(f"æ²¡æœ‰æ‰¾åˆ°æ—¥æœŸ {latest_date.date()} çš„å› å­æ•°æ®")
    
    print(f"  æœ€æ–°æ—¥æœŸå› å­æ•°æ®: {len(latest_factors)} åªè‚¡ç¥¨")
    
    # åŠ è½½æ¨¡å‹
    import lightgbm as lgb
    import json
    
    if ranker_path.exists():
        print(f"  åŠ è½½æ¨¡å‹: {ranker_path}")
        model = lgb.Booster(model_file=str(ranker_path))
        feature_list_path = model_dir / "feature_list_ranker.json"
    elif reg_path.exists():
        print(f"  åŠ è½½æ¨¡å‹: {reg_path}")
        model = lgb.Booster(model_file=str(reg_path))
        feature_list_path = model_dir / "feature_list_regression.json"
    else:
        raise FileNotFoundError("æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶")
    
    # åŠ è½½ç‰¹å¾åˆ—è¡¨
    if feature_list_path.exists():
        with open(feature_list_path, "r") as f:
            feature_list = json.load(f)
        print(f"  åŠ è½½ç‰¹å¾åˆ—è¡¨: {len(feature_list)} ä¸ªç‰¹å¾")
    else:
        # ä»æ¨¡å‹è·å–ç‰¹å¾å
        feature_list = model.feature_name()
        print(f"  ä»æ¨¡å‹è·å–ç‰¹å¾åˆ—è¡¨: {len(feature_list)} ä¸ªç‰¹å¾")
    
    # ç‰¹å¾å¯¹é½
    print("  å¯¹é½ç‰¹å¾...")
    available_features = set(latest_factors.columns)
    required_features = set(feature_list)
    
    missing_features = required_features - available_features
    extra_features = available_features - required_features
    
    if missing_features:
        print(f"  âš ï¸ ç¼ºå¤±ç‰¹å¾ {len(missing_features)} ä¸ªï¼Œç”¨ä¸­ä½æ•°å¡«å……")
        # å°è¯•ä»å†å²æ•°æ®è·å–ä¸­ä½æ•°
        if factors_path.exists():
            hist_factors = pd.read_parquet(factors_path)
            for feat in missing_features:
                if feat in hist_factors.columns:
                    median_val = hist_factors[feat].median()
                    latest_factors[feat] = median_val
                else:
                    latest_factors[feat] = 0.0
    
    # åªä¿ç•™éœ€è¦çš„ç‰¹å¾ï¼ŒæŒ‰é¡ºåºæ’åˆ—
    X_pred = latest_factors.reindex(columns=feature_list, fill_value=0.0)
    
    # å¡«å……ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨ä¸­ä½æ•°ï¼‰
    for col in X_pred.columns:
        if X_pred[col].isna().any():
            median_val = X_pred[col].median()
            if pd.isna(median_val):
                median_val = 0.0
            X_pred[col] = X_pred[col].fillna(median_val)
    
    print(f"  é¢„æµ‹æ•°æ®å½¢çŠ¶: {X_pred.shape}")
    
    # ç”Ÿæˆé¢„æµ‹
    print("  ç”Ÿæˆé¢„æµ‹...")
    pred_values = model.predict(X_pred.values)
    
    # åˆ›å»ºé¢„æµ‹ Series
    pred_series = pd.Series(
        pred_values,
        index=pd.MultiIndex.from_tuples(
            [(latest_date, ticker) for ticker in latest_factors.index.get_level_values("ticker")],
            names=["date", "ticker"]
        ),
        name="prediction"
    )
    
    print(f"  âœ“ é¢„æµ‹å®Œæˆ: {len(pred_series)} ä¸ªæ ·æœ¬")
    print(f"    é¢„æµ‹å€¼èŒƒå›´: [{pred_series.min():.4f}, {pred_series.max():.4f}]")
    
    # ä¿å­˜é¢„æµ‹ï¼ˆè¿½åŠ åˆ°ç°æœ‰é¢„æµ‹æ–‡ä»¶ï¼‰
    pred_file = model_dir / f"{model_type}_predictions.pkl"
    if pred_file.exists():
        import pickle
        with open(pred_file, "rb") as f:
            existing_pred = pickle.load(f)
        
        if isinstance(existing_pred, pd.Series):
            # ç§»é™¤åŒä¸€å¤©çš„æ—§é¢„æµ‹ï¼ˆå¦‚æœæœ‰ï¼‰
            existing_pred = existing_pred.loc[
                existing_pred.index.get_level_values("date") != latest_date
            ]
            # åˆå¹¶
            combined_pred = pd.concat([existing_pred, pred_series]).sort_index()
        else:
            combined_pred = pred_series
    else:
        combined_pred = pred_series
    
    import pickle
    with open(pred_file, "wb") as f:
        pickle.dump(combined_pred, f)
    
    print(f"  âœ“ å·²ä¿å­˜é¢„æµ‹åˆ° {pred_file}")
    
    return pred_series


def generate_daily_weights(cfg, pred_series: pd.Series):
    """ç”Ÿæˆæœ€æ–°æ—¥æœŸçš„æƒé‡"""
    print("\n" + "=" * 60)
    print("[Step 4] ç”Ÿæˆæƒé‡")
    print("=" * 60)
    
    weights_path = Path(cfg["paths"]["portfolio_path"])
    strategy_config = cfg.get("strategy", {})
    
    # è·å–æœ€æ–°æ—¥æœŸçš„é¢„æµ‹
    latest_date = pred_series.index.get_level_values("date").max()
    pred_day = pred_series.xs(latest_date, level="date").dropna()
    
    # è¿‡æ»¤åˆ°å‰100è‚¡ç¥¨ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    top100_tickers = load_top100_tickers()
    if top100_tickers:
        available_tickers = set(pred_day.index)
        valid_tickers = [t for t in top100_tickers if t in available_tickers]
        if len(valid_tickers) < len(top100_tickers):
            missing = set(top100_tickers) - available_tickers
            print(f"  [Warn] {len(missing)} åªå‰100è‚¡ç¥¨åœ¨é¢„æµ‹æ•°æ®ä¸­ä¸å­˜åœ¨: {sorted(list(missing))[:10]}...")
        pred_day = pred_day.reindex(valid_tickers).dropna()
        print(f"  [Info] è¿‡æ»¤åˆ°å‰100è‚¡ç¥¨: {len(pred_day)} åªè‚¡ç¥¨")
    
    print(f"  æœ€æ–°æ—¥æœŸ: {latest_date.date()}")
    print(f"  é¢„æµ‹è‚¡ç¥¨æ•°: {len(pred_day)}")
    
    # è·å–å½“å‰æŒä»“ï¼ˆä»ç°æœ‰æƒé‡æ–‡ä»¶ï¼‰
    current_positions = pd.Series(dtype=float)
    if weights_path.exists():
        existing_weights = pd.read_parquet(weights_path)
        if latest_date in existing_weights.index:
            # è·å–å‰ä¸€å¤©çš„æŒä»“
            prev_dates = existing_weights.index[existing_weights.index < latest_date]
            if len(prev_dates) > 0:
                prev_date = prev_dates.max()
                current_positions = existing_weights.loc[prev_date].fillna(0.0)
                current_positions = current_positions[current_positions > 0]
                print(f"  å‰ä¸€æ—¥æŒä»“: {len(current_positions)} åªè‚¡ç¥¨")
    
    # ç”Ÿæˆæƒé‡
    strategy_type = strategy_config.get("type", "topk_dropout")
    topk = strategy_config.get("topk", 20)
    
    if strategy_type == "full_rebalance":
        print(f"  ç­–ç•¥: Full Rebalance (æ¯æ—¥å…¨é‡æ¢ä»“, topk={topk})")
        
        new_weights = full_rebalance_strategy(
            pred_day,
            current_positions,
            topk=topk,
        )
    elif strategy_type == "topk_dropout":
        n_drop = strategy_config.get("n_drop", 3)
        method_sell = strategy_config.get("method_sell", "bottom")
        method_buy = strategy_config.get("method_buy", "top")
        
        print(f"  ç­–ç•¥: TopK Dropout (topk={topk}, n_drop={n_drop})")
        
        new_weights = topk_dropout_strategy(
            pred_day,
            current_positions,
            topk=topk,
            n_drop=n_drop,
            method_sell=method_sell,
            method_buy=method_buy,
        )
    else:
        raise NotImplementedError(f"ç­–ç•¥ç±»å‹ {strategy_type} æš‚ä¸æ”¯æŒæ¯æ—¥æ›´æ–°")
    
    # å½’ä¸€åŒ–
    if new_weights.sum() > 0:
        new_weights = new_weights / new_weights.sum()
    
    print(f"  âœ“ ç”Ÿæˆæƒé‡: {len(new_weights[new_weights > 0])} åªè‚¡ç¥¨")
    print(f"    æƒé‡å’Œ: {new_weights.sum():.6f}")
    
    # ä¿å­˜æƒé‡ï¼ˆè¿½åŠ åˆ°ç°æœ‰æƒé‡æ–‡ä»¶ï¼‰
    if weights_path.exists():
        existing_weights = pd.read_parquet(weights_path)
        # ç§»é™¤åŒä¸€å¤©çš„æ—§æƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
        existing_weights = existing_weights.loc[existing_weights.index != latest_date]
        # æ·»åŠ æ–°æƒé‡
        new_weights_df = pd.DataFrame({latest_date: new_weights}).T
        combined_weights = pd.concat([existing_weights, new_weights_df]).sort_index()
    else:
        combined_weights = pd.DataFrame({latest_date: new_weights}).T
    
    weights_path.parent.mkdir(parents=True, exist_ok=True)
    combined_weights.to_parquet(weights_path)
    
    print(f"  âœ“ å·²ä¿å­˜æƒé‡åˆ° {weights_path}")
    print(f"    æƒé‡æ–‡ä»¶æ—¥æœŸèŒƒå›´: {combined_weights.index.min().date()} åˆ° {combined_weights.index.max().date()}")
    
    return new_weights


def main():
    parser = argparse.ArgumentParser(description="æ¯æ—¥æ›´æ–°ï¼šè·å–æœ€æ–°æ•°æ®ã€è®¡ç®—å› å­ã€ç”Ÿæˆé¢„æµ‹å’Œæƒé‡")
    parser.add_argument("--lookback-days", type=int, default=60,
                       help="è·å–å¤šå°‘å¤©çš„å†å²æ•°æ®ï¼ˆé»˜è®¤ 60ï¼Œå› å­è®¡ç®—éœ€è¦ï¼‰")
    parser.add_argument("--model-type", default="lightgbm", choices=["lightgbm", "catboost", "xgboost"],
                       help="æ¨¡å‹ç±»å‹ï¼ˆé»˜è®¤ lightgbmï¼‰")
    parser.add_argument("--skip-fetch", action="store_true",
                       help="è·³è¿‡æ•°æ®è·å–ï¼ˆå¦‚æœå·²ç»æ‰‹åŠ¨æ›´æ–°äº†æ•°æ®ï¼‰")
    parser.add_argument("--skip-factors", action="store_true",
                       help="è·³è¿‡å› å­è®¡ç®—ï¼ˆå¦‚æœå› å­å·²ç»è®¡ç®—å¥½ï¼‰")
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    cfg = load_settings(SETTINGS)
    
    try:
        # Step 1: æ›´æ–°ä»·æ ¼æ•°æ®
        if not args.skip_fetch:
            update_prices(cfg, args.lookback_days)
        else:
            print("[è·³è¿‡] æ•°æ®è·å–")
        
        # Step 2: æ›´æ–°å› å­
        if not args.skip_factors:
            update_factors(cfg, args.lookback_days)
        else:
            print("[è·³è¿‡] å› å­è®¡ç®—")
        
        # Step 3: ç”Ÿæˆé¢„æµ‹
        pred_series = generate_daily_prediction(cfg, args.model_type)
        
        # Step 4: ç”Ÿæˆæƒé‡
        weights = generate_daily_weights(cfg, pred_series)
        
        print("\n" + "=" * 60)
        print("âœ“ æ¯æ—¥æ›´æ–°å®Œæˆï¼")
        print("=" * 60)
        print(f"  æœ€æ–°æ—¥æœŸ: {pred_series.index.get_level_values('date').max().date()}")
        print(f"  æŒä»“è‚¡ç¥¨æ•°: {len(weights[weights > 0])}")
        print(f"  æƒé‡æ–‡ä»¶: {cfg['paths']['portfolio_path']}")
        print("\nä¸‹ä¸€æ­¥ï¼š")
        print("  1. æŸ¥çœ‹æƒé‡: python -c \"import pandas as pd; print(pd.read_parquet('outputs/portfolios/weights.parquet').tail(1))\"")
        print("  2. è¿è¡Œå›æµ‹: python src/backtest.py --run")
        print("  3. å®ç›˜äº¤æ˜“: python src/ibkr_live_trader.py --weights outputs/portfolios/weights.parquet")
        
    except Exception as e:
        print(f"\nâœ— æ›´æ–°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

