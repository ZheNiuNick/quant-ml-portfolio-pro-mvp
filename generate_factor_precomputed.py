#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é¢„è®¡ç®—å› å­è¯Šæ–­ç»“æœï¼Œé¿å…åœ¨ Web éƒ¨ç½²æ—¶è¯»å–å¤§æ–‡ä»¶
ç”Ÿæˆï¼š
- outputs/factor_long_short.json - Long-Short æ”¶ç›Šï¼ˆæŒ‰å› å­ï¼‰
- outputs/factor_corr.json - å› å­ç›¸å…³æ€§çŸ©é˜µ
- outputs/factor_exposure.json - é£é™©æš´éœ²ï¼ˆæŒ‰æ—¥æœŸï¼‰
"""

import json
import sys
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np

# ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„ç®¡ç†
sys.path.insert(0, str(Path(__file__).parent))

from src.config.path import SETTINGS_FILE, OUTPUT_DIR, DATA_FACTORS_DIR, ROOT_DIR, get_path
from src.factor_engine import read_prices, forward_return, load_settings as load_factor_settings

SETTINGS = SETTINGS_FILE
OUTPUT_DIR.mkdir(exist_ok=True)

def generate_long_short_performance():
    """ç”Ÿæˆ Long-Short æ”¶ç›Šæ•°æ®ï¼ˆæŒ‰å› å­ï¼‰"""
    print("=" * 60)
    print("ç”Ÿæˆ Long-Short æ”¶ç›Šæ•°æ®...")
    print("=" * 60)
    
    try:
        from src.factor_engine import read_prices, forward_return
        
        # è¯»å–é…ç½®
        factor_cfg = load_factor_settings(str(SETTINGS))
        factor_store_rel_path = factor_cfg["paths"].get("factors_store", "data/factors/factor_store.parquet")
        # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™åŸºäºé¡¹ç›®æ ¹ç›®å½•è§£æ
        if Path(factor_store_rel_path).is_absolute():
            factor_store_path = Path(factor_store_rel_path)
        else:
            from src.config.path import ROOT_DIR
            factor_store_path = (ROOT_DIR / factor_store_rel_path).resolve()
        
        if not factor_store_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {factor_store_path}")
            return False
        
        print(f"ğŸ“– è¯»å–å› å­æ•°æ®: {factor_store_path}")
        factor_store = pd.read_parquet(factor_store_path)
        
        if not isinstance(factor_store.index, pd.MultiIndex):
            if "date" in factor_store.columns and "ticker" in factor_store.columns:
                factor_store["date"] = pd.to_datetime(factor_store["date"])
                factor_store = factor_store.set_index(["date", "ticker"]).sort_index()
        
        # è¯»å–ä»·æ ¼æ•°æ®
        print("ğŸ“– è¯»å–ä»·æ ¼æ•°æ®...")
        if "paths" in factor_cfg and "prices_parquet" in factor_cfg["paths"]:
            parquet_path = factor_cfg["paths"]["prices_parquet"]
            factor_cfg["paths"]["prices_parquet"] = str(get_path(parquet_path))
        
        prices = read_prices(factor_cfg)
        if prices is None or len(prices) == 0:
            print("âŒ ä»·æ ¼æ•°æ®ä¸å­˜åœ¨æˆ–ä¸ºç©º")
            return False
        
        # å¤„ç†é‡å¤ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if isinstance(prices.index, pd.MultiIndex):
            prices = prices[~prices.index.duplicated(keep='first')]
            print(f"ğŸ“Š ä»·æ ¼æ•°æ®å»é‡å: {len(prices)} è¡Œ")
        
        # è®¡ç®—æœªæ¥æ”¶ç›Š
        forward_ret = forward_return(prices, horizon=1)
        
        # å¤„ç† forward_ret çš„é‡å¤ç´¢å¼•
        if isinstance(forward_ret.index, pd.MultiIndex):
            forward_ret = forward_ret[~forward_ret.index.duplicated(keep='first')]
        
        # è·å–æ‰€æœ‰å› å­
        factors = [col for col in factor_store.columns if col not in ['date', 'ticker']]
        print(f"ğŸ“Š å¤„ç† {len(factors)} ä¸ªå› å­...")
        
        results = {}
        
        for i, factor_name in enumerate(factors, 1):
            if i % 10 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(factors)}")
            
            try:
                # è·å–è¿‘12ä¸ªæœˆçš„æ•°æ®
                latest_date = factor_store.index.get_level_values(0).max()
                start_date = latest_date - pd.DateOffset(months=12)
                date_range = factor_store.index.get_level_values(0).unique()
                date_range = date_range[date_range >= start_date]
                
                if len(date_range) == 0:
                    date_range = factor_store.index.get_level_values(0).unique()
                
                dates = []
                long_returns = []
                short_returns = []
                long_short_returns = []
                
                for date in sorted(date_range):
                    date_factors = factor_store.loc[factor_store.index.get_level_values(0) == date, factor_name]
                    # å¤„ç†é‡å¤ç´¢å¼•
                    if isinstance(date_factors.index, pd.MultiIndex):
                        date_factors = date_factors[~date_factors.index.duplicated(keep='first')]
                    
                    date_forward_ret = forward_ret.loc[forward_ret.index.get_level_values(0) == date]
                    # å¤„ç†é‡å¤ç´¢å¼•
                    if isinstance(date_forward_ret.index, pd.MultiIndex):
                        date_forward_ret = date_forward_ret[~date_forward_ret.index.duplicated(keep='first')]
                    
                    aligned = pd.concat([date_factors, date_forward_ret], axis=1).dropna()
                    if len(aligned) < 20:
                        continue
                    
                    aligned = aligned.sort_values(by=aligned.columns[0])
                    n = len(aligned)
                    long_portfolio = aligned.iloc[-n//5:]
                    short_portfolio = aligned.iloc[:n//5]
                    
                    long_ret = long_portfolio.iloc[:, 1].mean()
                    short_ret = short_portfolio.iloc[:, 1].mean()
                    ls_ret = long_ret - short_ret
                    
                    dates.append(date.strftime("%Y-%m-%d"))
                    long_returns.append(float(long_ret))
                    short_returns.append(float(short_ret))
                    long_short_returns.append(float(ls_ret))
                
                if len(dates) > 0:
                    # è®¡ç®—ç´¯è®¡æ”¶ç›Š
                    long_cum = (1 + pd.Series(long_returns)).cumprod().tolist()
                    short_cum = (1 + pd.Series(short_returns)).cumprod().tolist()
                    ls_cum = (1 + pd.Series(long_short_returns)).cumprod().tolist()
                    
                    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                    def calc_stats(returns):
                        returns_series = pd.Series(returns)
                        annual_return = returns_series.mean() * 252
                        sharpe = returns_series.mean() / returns_series.std() * np.sqrt(252) if returns_series.std() > 0 else 0
                        cum = (1 + returns_series).cumprod()
                        max_dd = (cum / cum.cummax() - 1).min()
                        return {
                            "annual_return": float(annual_return),
                            "sharpe": float(sharpe),
                            "max_dd": float(max_dd)
                        }
                    
                    results[factor_name] = {
                        "dates": dates,
                        "long_returns": long_cum,
                        "short_returns": short_cum,
                        "long_short_returns": ls_cum,
                        "stats": {
                            "long": calc_stats(long_returns),
                            "short": calc_stats(short_returns),
                            "long_short": calc_stats(long_short_returns)
                        }
                    }
            except Exception as e:
                print(f"  âš ï¸  å› å­ {factor_name} å¤„ç†å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        output_file = OUTPUT_DIR / "factor_long_short.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        file_size = output_file.stat().st_size / (1024 * 1024)
        print(f"âœ… ç”ŸæˆæˆåŠŸ: {output_file} ({file_size:.2f} MB, {len(results)} ä¸ªå› å­)")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def generate_correlation_matrix():
    """ç”Ÿæˆå› å­ç›¸å…³æ€§çŸ©é˜µ"""
    print("\n" + "=" * 60)
    print("ç”Ÿæˆå› å­ç›¸å…³æ€§çŸ©é˜µ...")
    print("=" * 60)
    
    try:
        factor_cfg = load_factor_settings(str(SETTINGS))
        factor_store_rel_path = factor_cfg["paths"].get("factors_store", "data/factors/factor_store.parquet")
        # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™åŸºäºé¡¹ç›®æ ¹ç›®å½•è§£æ
        if Path(factor_store_rel_path).is_absolute():
            factor_store_path = Path(factor_store_rel_path)
        else:
            factor_store_path = (ROOT_DIR / factor_store_rel_path).resolve()
        
        if not factor_store_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {factor_store_path}")
            return False
        
        print(f"ğŸ“– è¯»å–å› å­æ•°æ®: {factor_store_path}")
        factor_store = pd.read_parquet(factor_store_path)
        
        if not isinstance(factor_store.index, pd.MultiIndex):
            if "date" in factor_store.columns and "ticker" in factor_store.columns:
                factor_store["date"] = pd.to_datetime(factor_store["date"])
                factor_store = factor_store.set_index(["date", "ticker"]).sort_index()
        
        # è·å–è¿‘12ä¸ªæœˆçš„æ•°æ®
        latest_date = factor_store.index.get_level_values(0).max()
        start_date = latest_date - pd.DateOffset(months=12)
        recent_factors = factor_store.loc[factor_store.index.get_level_values(0) >= start_date]
        
        if len(recent_factors) == 0:
            recent_factors = factor_store
        
        # é€‰æ‹©éƒ¨åˆ†å› å­ï¼ˆé™åˆ¶ä¸º50ä¸ªï¼‰
        factors = list(recent_factors.columns)[:50]
        factor_subset = recent_factors[factors]
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µï¼ˆæŒ‰æ—¥æœŸå¹³å‡ï¼‰
        dates = factor_subset.index.get_level_values(0).unique()
        corr_list = []
        
        print(f"ğŸ“Š è®¡ç®— {len(factors)} ä¸ªå› å­çš„ç›¸å…³æ€§...")
        for i, date in enumerate(dates, 1):
            if i % 50 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(dates)}")
            date_factors = factor_subset.loc[factor_subset.index.get_level_values(0) == date]
            if len(date_factors) > 10:
                corr = date_factors.corr(method='pearson')
                corr_list.append(corr)
        
        if len(corr_list) == 0:
            print("âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—ç›¸å…³æ€§")
            return False
        
        # å¹³å‡ç›¸å…³æ€§çŸ©é˜µ
        mean_corr = pd.concat(corr_list).groupby(level=0).mean()
        mean_corr = mean_corr.fillna(0)
        
        # ä¿å­˜ç»“æœ
        output_file = OUTPUT_DIR / "factor_corr.json"
        result = {
            "factors": factors,
            "correlation_matrix": mean_corr.values.tolist(),
            "method": "pearson"
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        file_size = output_file.stat().st_size / (1024 * 1024)
        print(f"âœ… ç”ŸæˆæˆåŠŸ: {output_file} ({file_size:.2f} MB, {len(factors)} ä¸ªå› å­)")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def generate_risk_exposure():
    """ç”Ÿæˆé£é™©æš´éœ²æ•°æ®ï¼ˆæŒ‰æ—¥æœŸï¼‰"""
    print("\n" + "=" * 60)
    print("ç”Ÿæˆé£é™©æš´éœ²æ•°æ®...")
    print("=" * 60)
    
    try:
        factor_cfg = load_factor_settings(str(SETTINGS))
        factor_store_rel_path = factor_cfg["paths"].get("factors_store", "data/factors/factor_store.parquet")
        # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™åŸºäºé¡¹ç›®æ ¹ç›®å½•è§£æ
        if Path(factor_store_rel_path).is_absolute():
            factor_store_path = Path(factor_store_rel_path)
        else:
            factor_store_path = (ROOT_DIR / factor_store_rel_path).resolve()
        
        if not factor_store_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {factor_store_path}")
            return False
        
        print(f"ğŸ“– è¯»å–å› å­æ•°æ®: {factor_store_path}")
        factor_store = pd.read_parquet(factor_store_path)
        
        if not isinstance(factor_store.index, pd.MultiIndex):
            if "date" in factor_store.columns and "ticker" in factor_store.columns:
                factor_store["date"] = pd.to_datetime(factor_store["date"])
                factor_store = factor_store.set_index(["date", "ticker"]).sort_index()
        
        # è·å–æ‰€æœ‰å¯ç”¨æ—¥æœŸï¼ˆæœ€è¿‘30ä¸ªäº¤æ˜“æ—¥ï¼‰
        available_dates = pd.to_datetime(factor_store.index.get_level_values(0).unique()).sort_values()
        dates_to_process = available_dates[-30:]  # åªå¤„ç†æœ€è¿‘30ä¸ªäº¤æ˜“æ—¥
        
        print(f"ğŸ“Š å¤„ç† {len(dates_to_process)} ä¸ªæ—¥æœŸ...")
        
        results = {}
        
        for i, date_obj in enumerate(dates_to_process, 1):
            if i % 10 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(dates_to_process)}")
            
            try:
                date_factors = factor_store.loc[factor_store.index.get_level_values(0) == date_obj]
                
                # è®¡ç®—å› å­æš´éœ²åº¦ï¼ˆæ ‡å‡†åŒ–åçš„å› å­å€¼ï¼‰
                exposures = {}
                for factor_name in date_factors.columns:
                    factor_series = date_factors[factor_name].dropna()
                    if len(factor_series) > 0:
                        mean_val = factor_series.mean()
                        std_val = factor_series.std()
                        if std_val > 0:
                            exposures[factor_name] = (factor_series - mean_val) / std_val
                        else:
                            exposures[factor_name] = pd.Series([0.0] * len(factor_series), index=factor_series.index)
                
                # è®¡ç®—é£é™©è´¡çŒ®
                risk_contributions = {}
                total_risk = 0
                for factor_name, exp_series in exposures.items():
                    risk = exp_series.var()
                    risk_contributions[factor_name] = risk
                    total_risk += risk
                
                # å½’ä¸€åŒ–é£é™©è´¡çŒ®
                if total_risk > 0:
                    for factor_name in risk_contributions:
                        risk_contributions[factor_name] = risk_contributions[factor_name] / total_risk
                
                # è®¡ç®—æš´éœ²åº¦ï¼ˆä½¿ç”¨ä¸­ä½æ•°è€Œä¸æ˜¯å‡å€¼ï¼Œå› ä¸ºæ ‡å‡†åŒ–åçš„å› å­å‡å€¼ä¸º0ï¼‰
                # ä¸­ä½æ•°èƒ½æ›´å¥½åœ°åæ˜ å› å­å€¼çš„åˆ†å¸ƒç‰¹å¾ï¼Œä¿ç•™æ­£è´Ÿå·
                avg_exposures = {name: float(exp.median()) for name, exp in exposures.items()}
                
                # æ’åºï¼ˆæŒ‰é£é™©è´¡çŒ®ï¼Œå–å‰50ä¸ªä»¥æ˜¾ç¤ºæ›´å¤šå› å­ï¼ŒåŒ…æ‹¬æ­£è´Ÿæš´éœ²ï¼‰
                sorted_factors = sorted(risk_contributions.items(), key=lambda x: x[1], reverse=True)[:50]
                
                results[date_obj.strftime("%Y-%m-%d")] = {
                    "factors": [f[0] for f in sorted_factors],
                    "exposures": [round(avg_exposures.get(f[0], 0.0), 4) for f in sorted_factors],
                    "risk_contributions": [round(f[1] * 100, 2) for f in sorted_factors]
                }
            except Exception as e:
                print(f"  âš ï¸  æ—¥æœŸ {date_obj} å¤„ç†å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        output_file = OUTPUT_DIR / "factor_exposure.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        file_size = output_file.stat().st_size / (1024 * 1024)
        print(f"âœ… ç”ŸæˆæˆåŠŸ: {output_file} ({file_size:.2f} MB, {len(results)} ä¸ªæ—¥æœŸ)")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ç”Ÿæˆé¢„è®¡ç®—çš„å› å­è¯Šæ–­ç»“æœ...")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    success_count = 0
    
    if generate_long_short_performance():
        success_count += 1
    
    if generate_correlation_matrix():
        success_count += 1
    
    if generate_risk_exposure():
        success_count += 1
    
    print("\n" + "=" * 60)
    print(f"âœ… å®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count}/3 ä¸ªæ–‡ä»¶")
    print("=" * 60)
    
    if success_count == 3:
        print("\nğŸ“ ä¸‹ä¸€æ­¥ï¼š")
        print("1. æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶å¤§å°")
        print("2. å¦‚æœæ–‡ä»¶å¤ªå¤§ï¼ˆ>50MBï¼‰ï¼Œè€ƒè™‘ä¸Šä¼ åˆ° Hugging Face")
        print("3. æäº¤æ–‡ä»¶åˆ° Git æˆ–ä¸Šä¼ åˆ° Hugging Face")
        print("4. éƒ¨ç½²åï¼ŒAPI å°†ç›´æ¥è¯»å–è¿™äº› JSON æ–‡ä»¶")

