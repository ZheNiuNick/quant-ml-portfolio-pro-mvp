#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é¢„è®¡ç®—å› å­è¯Šæ–­ç»“æœï¼Œé¿å…åœ¨ Web éƒ¨ç½²æ—¶è¯»å–å¤§æ–‡ä»¶
ç”Ÿæˆï¼š
- outputs/factor_long_short.json - Long-Short æ”¶ç›Šï¼ˆæŒ‰å› å­ï¼‰
- outputs/factor_corr.json - å› å­ç›¸å…³æ€§çŸ©é˜µ
- outputs/factor_exposure.json - é£é™©æš´éœ²ï¼ˆæŒ‰æ—¥æœŸï¼‰
"""

import json
import sys
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np

# ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„ç®¡ç†
sys.path.insert(0, str(Path(__file__).parent))

from src.config.path import SETTINGS_FILE, OUTPUT_DIR, DATA_FACTORS_DIR, ROOT_DIR, get_path
from src.factor_engine import read_prices, forward_return, load_settings as load_factor_settings

SETTINGS = SETTINGS_FILE
OUTPUT_DIR.mkdir(exist_ok=True)

def generate_long_short_performance():
    """ç”Ÿæˆ Long-Short æ”¶ç›Šæ•°æ®ï¼ˆæŒ‰å› å­ï¼‰"""
    print("=" * 60)
    print("ç”Ÿæˆ Long-Short æ”¶ç›Šæ•°æ®...")
    print("=" * 60)
    
    try:
        from src.factor_engine import read_prices, forward_return
        
        # è¯»å–é…ç½®
        factor_cfg = load_factor_settings(str(SETTINGS))
        factor_store_rel_path = factor_cfg["paths"].get("factors_store", "data/factors/factor_store.parquet")
        # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™åŸºäºé¡¹ç›®æ ¹ç›®å½•è§£æ
        if Path(factor_store_rel_path).is_absolute():
            factor_store_path = Path(factor_store_rel_path)
        else:
            from src.config.path import ROOT_DIR
            factor_store_path = (ROOT_DIR / factor_store_rel_path).resolve()
        
        if not factor_store_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {factor_store_path}")
            return False
        
        print(f"ğŸ“– è¯»å–å› å­æ•°æ®: {factor_store_path}")
        factor_store = pd.read_parquet(factor_store_path)
        
        if not isinstance(factor_store.index, pd.MultiIndex):
            if "date" in factor_store.columns and "ticker" in factor_store.columns:
                factor_store["date"] = pd.to_datetime(factor_store["date"])
                factor_store = factor_store.set_index(["date", "ticker"]).sort_index()
        
        # è¯»å–ä»·æ ¼æ•°æ®
        print("ğŸ“– è¯»å–ä»·æ ¼æ•°æ®...")
        if "paths" in factor_cfg and "prices_parquet" in factor_cfg["paths"]:
            parquet_path = factor_cfg["paths"]["prices_parquet"]
            factor_cfg["paths"]["prices_parquet"] = str(get_path(parquet_path))
        
        prices = read_prices(factor_cfg)
        if prices is None or len(prices) == 0:
            print("âŒ ä»·æ ¼æ•°æ®ä¸å­˜åœ¨æˆ–ä¸ºç©º")
            return False
        
        # å¤„ç†é‡å¤ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if isinstance(prices.index, pd.MultiIndex):
            prices = prices[~prices.index.duplicated(keep='first')]
            print(f"ğŸ“Š ä»·æ ¼æ•°æ®å»é‡å: {len(prices)} è¡Œ")
        
        # è®¡ç®—æœªæ¥æ”¶ç›Š
        forward_ret = forward_return(prices, horizon=1)
        
        # å¤„ç† forward_ret çš„é‡å¤ç´¢å¼•
        if isinstance(forward_ret.index, pd.MultiIndex):
            forward_ret = forward_ret[~forward_ret.index.duplicated(keep='first')]
        
        # è·å–æ‰€æœ‰å› å­
        factors = [col for col in factor_store.columns if col not in ['date', 'ticker']]
        print(f"ğŸ“Š å¤„ç† {len(factors)} ä¸ªå› å­...")
        
        results = {}
        
        for i, factor_name in enumerate(factors, 1):
            if i % 10 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(factors)}")
            
            try:
                # è·å–è¿‘12ä¸ªæœˆçš„æ•°æ®
                latest_date = factor_store.index.get_level_values(0).max()
                start_date = latest_date - pd.DateOffset(months=12)
                date_range = factor_store.index.get_level_values(0).unique()
                date_range = date_range[date_range >= start_date]
                
                if len(date_range) == 0:
                    date_range = factor_store.index.get_level_values(0).unique()
                
                dates = []
                long_returns = []
                short_returns = []
                long_short_returns = []
                
                for date in sorted(date_range):
                    date_factors = factor_store.loc[factor_store.index.get_level_values(0) == date, factor_name]
                    # å¤„ç†é‡å¤ç´¢å¼•
                    if isinstance(date_factors.index, pd.MultiIndex):
                        date_factors = date_factors[~date_factors.index.duplicated(keep='first')]
                    
                    date_forward_ret = forward_ret.loc[forward_ret.index.get_level_values(0) == date]
                    # å¤„ç†é‡å¤ç´¢å¼•
                    if isinstance(date_forward_ret.index, pd.MultiIndex):
                        date_forward_ret = date_forward_ret[~date_forward_ret.index.duplicated(keep='first')]
                    
                    aligned = pd.concat([date_factors, date_forward_ret], axis=1).dropna()
                    if len(aligned) < 20:
                        continue
                    
                    aligned = aligned.sort_values(by=aligned.columns[0])
                    n = len(aligned)
                    long_portfolio = aligned.iloc[-n//5:]
                    short_portfolio = aligned.iloc[:n//5]
                    
                    long_ret = long_portfolio.iloc[:, 1].mean()
                    short_ret = short_portfolio.iloc[:, 1].mean()
                    ls_ret = long_ret - short_ret
                    
                    dates.append(date.strftime("%Y-%m-%d"))
                    long_returns.append(float(long_ret))
                    short_returns.append(float(short_ret))
                    long_short_returns.append(float(ls_ret))
                
                if len(dates) > 0:
                    # è®¡ç®—ç´¯è®¡æ”¶ç›Š
                    long_cum = (1 + pd.Series(long_returns)).cumprod().tolist()
                    short_cum = (1 + pd.Series(short_returns)).cumprod().tolist()
                    ls_cum = (1 + pd.Series(long_short_returns)).cumprod().tolist()
                    
                    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                    def calc_stats(returns):
                        returns_series = pd.Series(returns)
                        annual_return = returns_series.mean() * 252
                        sharpe = returns_series.mean() / returns_series.std() * np.sqrt(252) if returns_series.std() > 0 else 0
                        cum = (1 + returns_series).cumprod()
                        max_dd = (cum / cum.cummax() - 1).min()
                        return {
                            "annual_return": float(annual_return),
                            "sharpe": float(sharpe),
                            "max_dd": float(max_dd)
                        }
                    
                    results[factor_name] = {
                        "dates": dates,
                        "long_returns": long_cum,
                        "short_returns": short_cum,
                        "long_short_returns": ls_cum,
                        "stats": {
                            "long": calc_stats(long_returns),
                            "short": calc_stats(short_returns),
                            "long_short": calc_stats(long_short_returns)
                        }
                    }
            except Exception as e:
                print(f"  âš ï¸  å› å­ {factor_name} å¤„ç†å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        output_file = OUTPUT_DIR / "factor_long_short.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        file_size = output_file.stat().st_size / (1024 * 1024)
        print(f"âœ… ç”ŸæˆæˆåŠŸ: {output_file} ({file_size:.2f} MB, {len(results)} ä¸ªå› å­)")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def generate_correlation_matrix():
    """ç”Ÿæˆå› å­ç›¸å…³æ€§çŸ©é˜µ"""
    print("\n" + "=" * 60)
    print("ç”Ÿæˆå› å­ç›¸å…³æ€§çŸ©é˜µ...")
    print("=" * 60)
    
    try:
        factor_cfg = load_factor_settings(str(SETTINGS))
        factor_store_rel_path = factor_cfg["paths"].get("factors_store", "data/factors/factor_store.parquet")
        # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™åŸºäºé¡¹ç›®æ ¹ç›®å½•è§£æ
        if Path(factor_store_rel_path).is_absolute():
            factor_store_path = Path(factor_store_rel_path)
        else:
            factor_store_path = (ROOT_DIR / factor_store_rel_path).resolve()
        
        if not factor_store_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {factor_store_path}")
            return False
        
        print(f"ğŸ“– è¯»å–å› å­æ•°æ®: {factor_store_path}")
        factor_store = pd.read_parquet(factor_store_path)
        
        if not isinstance(factor_store.index, pd.MultiIndex):
            if "date" in factor_store.columns and "ticker" in factor_store.columns:
                factor_store["date"] = pd.to_datetime(factor_store["date"])
                factor_store = factor_store.set_index(["date", "ticker"]).sort_index()
        
        # è·å–è¿‘12ä¸ªæœˆçš„æ•°æ®
        latest_date = factor_store.index.get_level_values(0).max()
        start_date = latest_date - pd.DateOffset(months=12)
        recent_factors = factor_store.loc[factor_store.index.get_level_values(0) >= start_date]
        
        if len(recent_factors) == 0:
            recent_factors = factor_store
        
        # é€‰æ‹©éƒ¨åˆ†å› å­ï¼ˆé™åˆ¶ä¸º50ä¸ªï¼‰
        factors = list(recent_factors.columns)[:50]
        factor_subset = recent_factors[factors]
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µï¼ˆæŒ‰æ—¥æœŸå¹³å‡ï¼‰
        dates = factor_subset.index.get_level_values(0).unique()
        corr_list = []
        
        print(f"ğŸ“Š è®¡ç®— {len(factors)} ä¸ªå› å­çš„ç›¸å…³æ€§...")
        for i, date in enumerate(dates, 1):
            if i % 50 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(dates)}")
            date_factors = factor_subset.loc[factor_subset.index.get_level_values(0) == date]
            if len(date_factors) > 10:
                corr = date_factors.corr(method='pearson')
                corr_list.append(corr)
        
        if len(corr_list) == 0:
            print("âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—ç›¸å…³æ€§")
            return False
        
        # å¹³å‡ç›¸å…³æ€§çŸ©é˜µ
        mean_corr = pd.concat(corr_list).groupby(level=0).mean()
        mean_corr = mean_corr.fillna(0)
        
        # ä¿å­˜ç»“æœ
        output_file = OUTPUT_DIR / "factor_corr.json"
        result = {
            "factors": factors,
            "correlation_matrix": mean_corr.values.tolist(),
            "method": "pearson"
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        file_size = output_file.stat().st_size / (1024 * 1024)
        print(f"âœ… ç”ŸæˆæˆåŠŸ: {output_file} ({file_size:.2f} MB, {len(factors)} ä¸ªå› å­)")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def generate_risk_exposure():
    """ç”Ÿæˆé£é™©æš´éœ²æ•°æ®ï¼ˆæŒ‰æ—¥æœŸï¼‰- åŸºäºå®é™…æŠ•èµ„ç»„åˆæƒé‡"""
    print("\n" + "=" * 60)
    print("ç”Ÿæˆé£é™©æš´éœ²æ•°æ®...")
    print("=" * 60)
    
    try:
        # 1. è¯»å–å› å­æ•°æ®
        factor_cfg = load_factor_settings(str(SETTINGS))
        factor_store_rel_path = factor_cfg["paths"].get("factors_store", "data/factors/factor_store.parquet")
        if Path(factor_store_rel_path).is_absolute():
            factor_store_path = Path(factor_store_rel_path)
        else:
            factor_store_path = (ROOT_DIR / factor_store_rel_path).resolve()
        
        if not factor_store_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {factor_store_path}")
            return False
        
        print(f"ğŸ“– è¯»å–å› å­æ•°æ®: {factor_store_path}")
        factor_store = pd.read_parquet(factor_store_path)
        
        if not isinstance(factor_store.index, pd.MultiIndex):
            if "date" in factor_store.columns and "ticker" in factor_store.columns:
                factor_store["date"] = pd.to_datetime(factor_store["date"])
                factor_store = factor_store.set_index(["date", "ticker"]).sort_index()
        
        # 2. è¯»å–æŠ•èµ„ç»„åˆæƒé‡
        portfolio_path_rel = factor_cfg["paths"].get("portfolio_path", "outputs/portfolios/weights.parquet")
        if Path(portfolio_path_rel).is_absolute():
            portfolio_path = Path(portfolio_path_rel)
        else:
            portfolio_path = (ROOT_DIR / portfolio_path_rel).resolve()
        
        if not portfolio_path.exists():
            print(f"âš ï¸  æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {portfolio_path}")
            print("   å°†ä½¿ç”¨æ‰€æœ‰è‚¡ç¥¨çš„å¹³å‡å› å­å€¼ï¼ˆä¸æ¨èï¼‰")
            weights_df = None
        else:
            print(f"ğŸ“– è¯»å–æŠ•èµ„ç»„åˆæƒé‡: {portfolio_path}")
            weights_df = pd.read_parquet(portfolio_path)
            weights_df.index = pd.to_datetime(weights_df.index)
            print(f"   æƒé‡æ—¥æœŸèŒƒå›´: {weights_df.index.min()} åˆ° {weights_df.index.max()}")
        
        # è·å–æ‰€æœ‰å¯ç”¨æ—¥æœŸï¼ˆæœ€è¿‘30ä¸ªäº¤æ˜“æ—¥ï¼‰
        available_dates = pd.to_datetime(factor_store.index.get_level_values(0).unique()).sort_values()
        dates_to_process = available_dates[-30:]  # åªå¤„ç†æœ€è¿‘30ä¸ªäº¤æ˜“æ—¥
        
        print(f"ğŸ“Š å¤„ç† {len(dates_to_process)} ä¸ªæ—¥æœŸ...")
        
        results = {}
        
        for i, date_obj in enumerate(dates_to_process, 1):
            if i % 10 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(dates_to_process)}")
            
            try:
                date_factors = factor_store.loc[factor_store.index.get_level_values(0) == date_obj]
                # é‡ç½®ç´¢å¼•ï¼Œåªä¿ç•™ ticker çº§åˆ«
                if isinstance(date_factors.index, pd.MultiIndex):
                    date_factors = date_factors.reset_index(level='date', drop=True)
                
                # 3. è·å–å½“æ—¥æŠ•èµ„ç»„åˆæƒé‡
                portfolio_weights = None
                portfolio_tickers = None
                if weights_df is not None:
                    # æŸ¥æ‰¾æœ€æ¥è¿‘çš„æ—¥æœŸæƒé‡
                    if date_obj in weights_df.index:
                        portfolio_weights_series = weights_df.loc[date_obj].fillna(0.0)
                        portfolio_weights_series = portfolio_weights_series[portfolio_weights_series > 0]
                        if len(portfolio_weights_series) > 0:
                            portfolio_weights = portfolio_weights_series
                            portfolio_tickers = portfolio_weights.index.tolist()
                            # å½’ä¸€åŒ–æƒé‡ï¼ˆç¡®ä¿å’Œä¸º1ï¼‰
                            portfolio_weights = portfolio_weights / portfolio_weights.sum()
                
                # 4. è®¡ç®—å› å­æš´éœ²åº¦å’Œé£é™©è´¡çŒ®
                portfolio_exposures = {}
                portfolio_risk_contributions = {}
                
                for factor_name in date_factors.columns:
                    # date_factors å·²ç»æ˜¯è¯¥æ—¥æœŸçš„æ•°æ®ï¼Œfactor_series æ˜¯ Series(ticker -> value)
                    factor_series = date_factors[factor_name].dropna()
                    
                    if portfolio_weights is not None and portfolio_tickers:
                        # æ–¹æ³•1: ä½¿ç”¨æŠ•èµ„ç»„åˆæƒé‡è®¡ç®—å› å­æš´éœ²åº¦ï¼ˆBarra-styleï¼‰
                        # åªè€ƒè™‘æŠ•èµ„ç»„åˆä¸­çš„è‚¡ç¥¨
                        portfolio_factor_values = factor_series.reindex(portfolio_tickers).dropna()
                        portfolio_weights_aligned = portfolio_weights.reindex(portfolio_factor_values.index).fillna(0.0)
                        
                        if len(portfolio_factor_values) > 0 and portfolio_weights_aligned.sum() > 0:
                            # å½’ä¸€åŒ–å¯¹é½çš„æƒé‡
                            portfolio_weights_aligned = portfolio_weights_aligned / portfolio_weights_aligned.sum()
                            
                            # è®¡ç®—åŸºå‡†ï¼ˆæ‰€æœ‰è‚¡ç¥¨çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
                            benchmark_mean = factor_series.mean()
                            benchmark_std = factor_series.std()
                            
                            # è®¡ç®—æŠ•èµ„ç»„åˆçš„å› å­æš´éœ²åº¦ï¼ˆåŠ æƒå¹³å‡ï¼‰
                            portfolio_factor_mean = (portfolio_weights_aligned * portfolio_factor_values).sum()
                            
                            # Barra-style æš´éœ²åº¦ï¼šç›¸å¯¹äºåŸºå‡†çš„æ ‡å‡†åŒ–æš´éœ²åº¦
                            # æš´éœ²åº¦ = (æŠ•èµ„ç»„åˆå› å­å€¼ - åŸºå‡†å› å­å€¼) / åŸºå‡†å› å­æ ‡å‡†å·®
                            if benchmark_std > 0:
                                portfolio_exposure = (portfolio_factor_mean - benchmark_mean) / benchmark_std
                            else:
                                portfolio_exposure = 0.0
                            
                            portfolio_exposures[factor_name] = float(portfolio_exposure)
                            
                            # è®¡ç®—é£é™©è´¡çŒ®ï¼šä½¿ç”¨æŠ•èµ„ç»„åˆä¸­è‚¡ç¥¨å› å­å€¼çš„åŠ æƒæ–¹å·®
                            weighted_mean = (portfolio_weights_aligned * portfolio_factor_values).sum()
                            weighted_variance = ((portfolio_weights_aligned * (portfolio_factor_values - weighted_mean) ** 2).sum())
                            portfolio_risk_contributions[factor_name] = weighted_variance
                        else:
                            portfolio_exposures[factor_name] = 0.0
                            portfolio_risk_contributions[factor_name] = 0.0
                    else:
                        # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æƒé‡æ–‡ä»¶ï¼Œä½¿ç”¨æ‰€æœ‰è‚¡ç¥¨çš„å¹³å‡ï¼ˆæ—§æ–¹æ³•ï¼‰
                        mean_val = factor_series.mean()
                        std_val = factor_series.std()
                        if std_val > 0:
                            normalized = (factor_series - mean_val) / std_val
                            portfolio_exposures[factor_name] = float(normalized.median())
                        else:
                            portfolio_exposures[factor_name] = 0.0
                        portfolio_risk_contributions[factor_name] = float(factor_series.var())
                
                # 5. å½’ä¸€åŒ–é£é™©è´¡çŒ®
                total_risk = sum(portfolio_risk_contributions.values())
                if total_risk > 0:
                    for factor_name in portfolio_risk_contributions:
                        portfolio_risk_contributions[factor_name] = portfolio_risk_contributions[factor_name] / total_risk
                
                # 6. æ’åºï¼ˆæŒ‰é£é™©è´¡çŒ®ï¼Œå–å‰50ä¸ªï¼‰
                sorted_factors = sorted(portfolio_risk_contributions.items(), key=lambda x: x[1], reverse=True)[:50]
                
                results[date_obj.strftime("%Y-%m-%d")] = {
                    "factors": [f[0] for f in sorted_factors],
                    "exposures": [round(portfolio_exposures.get(f[0], 0.0), 4) for f in sorted_factors],
                    "risk_contributions": [round(f[1] * 100, 2) for f in sorted_factors]
                }
            except Exception as e:
                print(f"  âš ï¸  æ—¥æœŸ {date_obj} å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # ä¿å­˜ç»“æœ
        output_file = OUTPUT_DIR / "factor_exposure.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        file_size = output_file.stat().st_size / (1024 * 1024)
        print(f"âœ… ç”ŸæˆæˆåŠŸ: {output_file} ({file_size:.2f} MB, {len(results)} ä¸ªæ—¥æœŸ)")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ç”Ÿæˆé¢„è®¡ç®—çš„å› å­è¯Šæ–­ç»“æœ...")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    success_count = 0
    
    if generate_long_short_performance():
        success_count += 1
    
    if generate_correlation_matrix():
        success_count += 1
    
    if generate_risk_exposure():
        success_count += 1
    
    print("\n" + "=" * 60)
    print(f"âœ… å®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count}/3 ä¸ªæ–‡ä»¶")
    print("=" * 60)
    
    if success_count == 3:
        print("\nğŸ“ ä¸‹ä¸€æ­¥ï¼š")
        print("1. æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶å¤§å°")
        print("2. å¦‚æœæ–‡ä»¶å¤ªå¤§ï¼ˆ>50MBï¼‰ï¼Œè€ƒè™‘ä¸Šä¼ åˆ° Hugging Face")
        print("3. æäº¤æ–‡ä»¶åˆ° Git æˆ–ä¸Šä¼ åˆ° Hugging Face")
        print("4. éƒ¨ç½²åï¼ŒAPI å°†ç›´æ¥è¯»å–è¿™äº› JSON æ–‡ä»¶")

